---
title: "Text as Data"
author: "Cory J. Cascalheira"
date: "6/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load dependencies - utility
library(tidyverse)
library(stringr)

# For text analysis
library(topicmodels)
library(tm)
library(tidytext)
library(stm)

# Stemming in tidytext
library(SnowballC)

# For removing URLs and weird language from text
#library(gdap)

# Load data
load(url("https://cbail.github.io/Trump_Tweets.Rdata"))
data("AssociatedPress")

# Setting up the data for STM
google_doc_id <- "1LcX-JnpGB0lU1iDnXnxB6WFqBywUKpew" # google file ID
poliblogs <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", google_doc_id), stringsAsFactors = FALSE)
```


## Text Data = Latent Variables

When we look at text, we often care about something else---an indirect, latent variable.

Text is an imperfect manifestation of some unobserved concept.

- Background concept = negativity / **minority stress**
- Systematized concept = negative emotion in politics represents an individual's orientation toward costs/benefits of a given issue 
  - How to operationalize this?
- Indicator = negative words minus positive words
- Scores = final scores

What is the difference between my concept and the data?

**Resources**

- quanteda.io
- tidytextmining.com 
- STM package
- TM package
- snsoroka.com/data-lexicoder (for political texts)

## Text Analysis Basics

**Character encoding** = numeric representation of textual symbols (e.g., unicode, UTF-8).

- Character encoding varies overtime
- Coerce characters into a specific numeric representation

**GREP** = globally search a regualr expression

- Regular expressions = pattern in strings

```{r}
# Base functions - locate string
text_chunk <- c("[this [IS a sentence[[[[")
grepl("IS", text_chunk)

# Base functions - replace string
gsub("\\[", "", text_chunk)
```

**Tokenization** = create a unit of analysis from textual data

- N-grams = sequences of words of length *n*
  - Unigrams
  - Bigrams
  - Trigrams

Problems with N-grams? Catch lots of non-sensical data

**Database Representation of Text** 

- Corpus = large dataset of documents
- Perserve meta data, full-text document, and matrix representation of textual document

**TidyText** = each word is a row

- Benefit of TidyText = keep text as df, then access to all tidyverse tools
- If using Corpus, then need to learn new syntax 
- Corpus = more standard
- TidyText = easier to learn, but harder with larger datasets (would need a cloud machine)

```{r}
# Example text
trumptweets$text[1]

# Define a corpus
trump_corpus <- Corpus(VectorSource(as.vector(trumptweets$text)))
trump_corpus

# Create tidy text df
tidy_trump_tweets <- trumptweets %>%
  # Unique identifers and text
  select(created_at, text) %>%
  unnest_tokens("word", text)

# Show units of analysis
head(tidy_trump_tweets)
```

**Text Pre-Processing**

- Remove stop words (e.g., prepositions)
- Punctuations removed automatically in tidytext
- Numbers usually do not add much to the analysis
- Need to standardized word case, but done automatically in tidytext
- Trim all the whitespace
- Stemming = reduce word to its most basic form 

```{r}
# How to remove English stopwords from a Corpus
trump_corpus <- tm_map(trump_corpus, removeWords, stopwords("english"))
trump_corpus

# How to remove English stopwords with tidytext
data("stop_words")
tidy_trump_tweets <- tidy_trump_tweets %>%
  anti_join(stop_words)

# Remove punctuation
trump_corpus <- tm_map(trump_corpus, content_transformer(removePunctuation))

# Remove numbers
trump_corpus <- tm_map(trump_corpus, content_transformer(removeNumbers))
tidy_trump_tweets <- tidy_trump_tweets[-grep("\\b\\d+\\b", tidy_trump_tweets$word), ]

# Standardized word case
trump_corpus <- tm_map(trump_corpus, content_transformer(tolower))

# Remove whitespace
trump_corpus <- tm_map(trump_corpus, content_transformer(stripWhitespace))
tidy_trump_tweets$word <- gsub("\\s+", "", tidy_trump_tweets$word)

# Perform stemming
trump_corpus <- tm_map(trump_corpus, content_transformer(stemDocument), language = "english")
tidy_trump_tweets <- tidy_trump_tweets %>%
  mutate(word = wordStem((word), language = "en"))
```

**Document-Term Matrix**

- Represents words across documents
- Then, can run functions similar to clsuter analysis and factor analysis to detect patterns

```{r}
# Make a DTM
trump_dtm <- DocumentTermMatrix(trump_corpus, control = list(wordLengths = c(2, Inf)))

# Check out the DTM
inspect(trump_dtm[1:5, 3:8])
```

## Dictionary-Based Text Analysis

- Sophisticated word counting based on content dictionaries
- Looking at top words give you a sense, but need to sue common words and uncommon words
- Create own dictionary (e.g., minority stressors from text of discriminatory experience)
- When to use?
  - If I know the words very well, then create a dictionary.
  - If exploratory, then use unsupervised methods (get a sense of the land / themes).

Thus, I may want to start with an unsupervised method first when detecting features of minority stress in textual data. Get a lay of the land first, then go back in and use dictionary analysis. Bail calls a hybrid approach "supervised."

```{r}
# Just the top words
top_words <- tidy_trump_tweets %>%
      anti_join(stop_words) %>%
        filter(!(word=="https"|
                 word=="rt"|
                 word=="t.co"|
                 word=="amp")) %>%
            count(word) %>%
              arrange(desc(n))

# Show graph of top words
top_words %>%
  # Remove the first 20 words
  slice(1:20) %>%
    ggplot(aes(x=reorder(word, -n), y=n, fill=word))+
      geom_bar(stat="identity")+
        theme_minimal()+
        theme(axis.text.x = 
            element_text(angle = 60, hjust = 1, size=13))+
        theme(plot.title = 
            element_text(hjust = 0.5, size=18))+
          ylab("Frequency")+
          xlab("")+
          ggtitle("Most Frequent Words in Trump Tweets")+
          guides(fill=FALSE)
```

**Inverse Document Frequency (IDF)**

- "tf-idf"
- Give more weight to a term occurring in less documents
- How unusual is a term within and between documents?
- These words add meaning because they are fewer---might want to pay attention to them. For example, some words will be highly related to minority stress, but might not appear most frequent in a corpus or tidytext df.

$$IDF(t) = log(\frac{|D|}{df(t)})$$

- t = term
- df(t) = document frequency of t
- |D| = number of documents

```{r}
# Calculate TF-IDF
tidy_trump_tfidf <- trumptweets %>%
    select(created_at,text) %>%
      unnest_tokens("word", text) %>%
        anti_join(stop_words) %>%
           count(word, created_at) %>%
              bind_tf_idf(word, created_at, n)

# What are the unusual words?
top_tfidf <- tidy_trump_tfidf %>%
  arrange(desc(tf_idf))

top_tfidf$word[1]
```


**Creating a Dictionary**

- Good if know a lot about the topic (e.g., experts)

```{r}
# Dictionary example
economic_dictionary <- c("economy","unemployment","trade","tariffs")

# Pull out tweets related to the economy
economic_tweets <- trumptweets[str_detect(trumptweets$text, paste(economic_dictionary, collapse="|")), ]

# Show economic tweets
head(economic_tweets$text, 2)
```

Unsupervised algorithms:

- topic modeling
- word embedding
- text networks

### Sentiment Analysis

Type of dictionary-based analysis. All kinds of sentiments can be coded.

When working with sentiments, need to read about the construction of the sentiment dictionary. Choose wisely, one that is close to what I intend to measure.

```{r}
# Example with tidytext
head(get_sentiments("bing"))

# Number of negative tweets
trump_tweet_sentiment <- tidy_trump_tweets %>%
  inner_join(get_sentiments("bing")) %>%
    count(created_at, sentiment) 

head(trump_tweet_sentiment)
```

Work with words and time.

```{r}
# Convert to datetime
tidy_trump_tweets$date <- as.Date(tidy_trump_tweets$created_at, 
                                          format="%Y-%m-%d %x")

# Only keep negative tweets and count them
trump_sentiment_plot <-
  tidy_trump_tweets %>%
    inner_join(get_sentiments("bing")) %>% 
      filter(sentiment=="negative") %>%
          count(date, sentiment)

# Negative tweets by day
ggplot(trump_sentiment_plot, aes(x=date, y=n))+
  geom_line(color="red", size=.5)+
    theme_minimal()+
    theme(axis.text.x = 
            element_text(angle = 60, hjust = 1, size=13))+
    theme(plot.title = 
            element_text(hjust = 0.5, size=18))+
      ylab("Number of Negative Words")+
      xlab("")+
      ggtitle("Negative Sentiment in Trump Tweets")+
      theme(aspect.ratio=1/4)
```

### Linguistic Inquiry Word Count

Psychometrics of text. Weights of every word that capture positive/negative, internal states, and so forth.

Typically a high-quality tool worth using, but the subtleties may not be captured. 

Note that false positives and false negative are common no matter what. 

Not sure which sentiment analysis to use? See Goncalves et al. (2013). Different sentiment dictionaries perform well in different contexts. The paper shows that no sentiment dictionary was as accurate as human coding.  

## Topic Modeling

Identify latent themes in a corpus (e.g., large group of text). Used to identify features in corpus. 

**If I am looking for features for the MS project, then topic modeling might be a helpful approach. I would probably need to reduce a large corpus to posts that I think are related to minority stress, and then perform topic modeling? A supervised labeling approach might be better because MS theory is well-established.**

### Latent Dirichlet Allocation (LDA)

- LDA requires us to specific the number of topics ahead of time. **This might not be effective for the MS project. For example, the 3 to 4 "classic" proximal stressors may not be discussed on social media platforms, especially since private traits are harder to estimate (hence why I will probably start with distal stressors).**
- Mixed membership modeling (e.g., words can belong to more than one topic).
- A *k* will need to be adjusted iteratively.
- If a topic seems to have a bunch of random words, then we have likely under- or overestimated the *k* number of topics.

REMEMBER: document-term matrix has each document as a row and each term (i.e., word) as a column.

Challenges with topic models, and cluster analyses in general, is the difficulty with interpretation. High-quality human validation is required. Also, a systematic approach is needed. Try a range of *k* values to reduce bias (i.e., choose the *k* with topics that meet your expectations).

With latent constructs, it is hard to determine which words are reliably associated with a topic. 

DO NOT LET THE ALGORITHM DO ALL THE WORK.

```{r}
# Create a topic model using LDA
AP_topic_model <- LDA(AssociatedPress, 
                      # Number of topics
                      k = 10,
                      # Make the LDA reproducible
                      control = list(seed = 321))

# Probability of each word being asoociated with a topic 
AP_topics <- tidy(AP_topic_model, matrix = "beta")

# Count top ten words
ap_top_terms <- 
  AP_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

# Plot the words by topic
ap_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

## Structural Topic Modeling

- Roberts et al. (2015) 
- Similar to LDA, but uses meta data from a document to improve classification.
- For example, STM allows the inclusion of covariates, which improve the probability of classification.
- Lots of useful packages.
- Can use goodness of fit measures to determine value of *k*.

**STM might be a good fit for my MS project. Added to lab notebook. What would be the covariates?**

Preprocess the data.

```{r}
# Preprocess - specify where meta data are located
processed <- textProcessor(poliblogs$documents, metadata = poliblogs)
```

Store each required component of the STM---documents, meta data, vocabulary---as separate object.

```{r}
# Create the components
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)

# Assign the components to objects
docs <- out$documents
vocab <- out$vocab
meta <-out$meta
```

Create the STM.

```{r}
# Create STM
First_STM <- stm(documents = out$documents, vocab = out$vocab,
              K = 10, 
              # Determine how meta data is used
              prevalence =~ rating + s(day),
              max.em.its = 75, data = out$meta,
              init.type = "Spectral", verbose = FALSE)

# Plot STM
plot(First_STM)
```

Now that the topics are created, we can expect them to learn more about the topics.

```{r}
findThoughts(First_STM, texts = poliblogs$documents,
     n = 2, topics = 3)
```

There are also ways to find the optimal number of *k*. However, given the size of the corpus, note that this operation takes a long time (i.e., see [this website for the code](https://sicss.io/2020/materials/day3-text-analysis/topic-modeling/rmarkdown/Topic_Modeling.html)).

It is also possible to examine the meta data. These steps are found on see [this website for the code](https://sicss.io/2020/materials/day3-text-analysis/topic-modeling/rmarkdown/Topic_Modeling.html).

- Possible to determine how the topics vary with the covariates (e.g., is the topic more likely to be spoken by democrats or republicans?).
- Show distribution of topics over time.
- Do not need to add ALL meta data; be selective. Add RICH meta data that:
  - (a) is likely to influence the construction of the topic; or
  - (b) may be an important predictor of the topics.

### Limitations of Topic Models

- LDA and STM are not good for short texts, such as tweets.
  - **Solutions** = check out stLDA-C (Tierney et al.) if using Twitter data. It will cluster topics AND users (i.e., users most at risk for MS experiences?)
- Danger = no idea what is in the corpus, then might mind false negatives, spurious patterns, bag-of-words assumption (i.e., word order does not matter).
  - Sometimes bag-of-words are not useful assumptions.

## Text Networks

**Network** = group of nodes where edges are connections between them.
- Draw edges between the words (i.e., nodes) or the authors.
- Author network = useful if I want to cluster people by the way they talk.

If I want to do this, then use the **textnets package**.