---
title: "Text as Data"
author: "Cory J. Cascalheira"
date: "6/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load dependencies - utility
library(tidyverse)
library(stringr)

# For text analysis
library(tm)
library(tidytext)

# Stemming in tidytext
library(SnowballC)

# For removing URLs and weird language from text
library(gdap)

# Load data
load(url("https://cbail.github.io/Trump_Tweets.Rdata"))
```

## Text Analysis Basics

**Character encoding** = numeric representation of textual symbols (e.g., unicode, UTF-8).

- Character encoding varies overtime
- Coerce characters into a specific numeric representation

**GREP** = globally search a regualr expression

- Regular expressions = pattern in strings

```{r}
# Base functions - locate string
text_chunk <- c("[this [IS a sentence[[[[")
grepl("IS", text_chunk)

# Base functions - replace string
gsub("\\[", "", text_chunk)
```

**Tokenization** = create a unit of analysis from textual data

- N-grams = sequences of words of length *n*
  - Unigrams
  - Bigrams
  - Trigrams

Problems with N-grams? Catch lots of non-sensical data

**Database Representation of Text** 

- Corpus = large dataset of documents
- Perserve meta data, full-text document, and matrix representation of textual document

**TidyText** = each word is a row

- Benefit of TidyText = keep text as df, then access to all tidyverse tools
- If using Corpus, then need to learn new syntax 
- Corpus = more standard
- TidyText = easier to learn, but harder with larger datasets (would need a cloud machine)

```{r}
# Example text
trumptweets$text[1]

# Define a corpus
trump_corpus <- Corpus(VectorSource(as.vector(trumptweets$text)))
trump_corpus

# Create tidy text df
tidy_trump_tweets <- trumptweets %>%
  # Unique identifers and text
  select(created_at, text) %>%
  unnest_tokens("word", text)

# Show units of analysis
head(tidy_trump_tweets)
```

**Text Pre-Processing**

- Remove stop words (e.g., prepositions)
- Punctuations removed automatically in tidytext
- Numbers usually do not add much to the analysis
- Need to standardized word case, but done automatically in tidytext
- Trim all the whitespace
- Stemming = reduce word to its most basic form 

```{r}
# How to remove English stopwords from a Corpus
trump_corpus <- tm_map(trump_corpus, removeWords, stopwords("english"))
trump_corpus

# How to remove English stopwords with tidytext
data("stop_words")
tidy_trump_tweets <- tidy_trump_tweets %>%
  anti_join(stop_words)

# Remove punctuation
trump_corpus <- tm_map(trump_corpus, content_transformer(removePunctuation))

# Remove numbers
trump_corpus <- tm_map(trump_corpus, content_transformer(removeNumbers))
tidy_trump_tweets <- tidy_trump_tweets[-grep("\\b\\d+\\b", tidy_trump_tweets$word), ]

# Standardized word case
trump_corpus <- tm_map(trump_corpus, content_transformer(tolower))

# Remove whitespace
trump_corpus <- tm_map(trump_corpus, content_transformer(stripWhitespace))
tidy_trump_tweets$word <- gsub("\\s+", "", tidy_trump_tweets$word)

# Perform stemming
trump_corpus <- tm_map(trump_corpus, content_transformer(stemDocument), language = "english")
tidy_trump_tweets <- tidy_trump_tweets %>%
  mutate(word = wordStem((word), language = "en"))
```

**Document-Term Matrix**

- Represents words across documents
- Then, can run functions similar to clsuter analysis and factor analysis to detect patterns

```{r}
# Make a DTM
trump_dtm <- DocumentTermMatrix(trump_corpus, control = list(wordLengths = c(2, Inf)))

# Check out the DTM
inspect(trump_dtm[1:5, 3:8])
```

## Dictionary-Based Text Analysis

- Sophisticated word counting based on content dictionaries
- Looking at top words give you a sense, but need to sue common words and uncommon words
- Create own dictionary (e.g., minority stressors from text of discriminatory experience)
- When to use?
  - If I know the words very well, then create a dictionary.
  - If exploratory, then use unsupervised methods (get a sense of the land / themes).

Thus, I may want to state with an unsupervised method first when detecting features of minority stress in textual data. Get a lay of the land first, then go back in and use dictionary analysis. Bail calls a hybrid approach "supervised."

```{r}
# Just the top words
top_words <- tidy_trump_tweets %>%
      anti_join(stop_words) %>%
        filter(!(word=="https"|
                 word=="rt"|
                 word=="t.co"|
                 word=="amp")) %>%
            count(word) %>%
              arrange(desc(n))

# Show graph of top words
top_words %>%
  # Remove the first 20 words
  slice(1:20) %>%
    ggplot(aes(x=reorder(word, -n), y=n, fill=word))+
      geom_bar(stat="identity")+
        theme_minimal()+
        theme(axis.text.x = 
            element_text(angle = 60, hjust = 1, size=13))+
        theme(plot.title = 
            element_text(hjust = 0.5, size=18))+
          ylab("Frequency")+
          xlab("")+
          ggtitle("Most Frequent Words in Trump Tweets")+
          guides(fill=FALSE)
```

**Inverse Document Frequency (IDF)**

- "tf-idf"
- Give more weight to a term occurring in less documents
- How unusual is a term within and between documents?
- These words add meaning because they are fewer---might want to pay attention to them. For example, some words will be highly related to minority stress, but might not appear most frequent in a corpus or tidytext df.

$$IDF(t) = log(\frac{|D|}{df(t)})$$

- t = term
- df(t) = document frequency of t
- |D| = number of documents

```{r}
# Calculate TF-IDF
tidy_trump_tfidf <- trumptweets %>%
    select(created_at,text) %>%
      unnest_tokens("word", text) %>%
        anti_join(stop_words) %>%
           count(word, created_at) %>%
              bind_tf_idf(word, created_at, n)

# What are the unusual words?
top_tfidf <- tidy_trump_tfidf %>%
  arrange(desc(tf_idf))

top_tfidf$word[1]
```


**Creating a Dictionary**

- Good if know a lot about the topic (e.g., experts)

```{r}
# Dictionary example
economic_dictionary <- c("economy","unemployment","trade","tariffs")

# Pull out tweets related to the economy
economic_tweets <- trumptweets[str_detect(trumptweets$text, paste(economic_dictionary, collapse="|")), ]

# Show economic tweets
head(economic_tweets$text, 2)
```

Unsupervised algorithms:

- topic modeling
- word embedding
- text networks

### Sentiment Analysis

Type of dictionary-based analysis. All kinds of sentiments can be coded.

When working with sentiments, need to read about the construction of the sentiment dictionary. Choose wisely, one that is close to what I intend to measure.

```{r}
# Example with tidytext
head(get_sentiments("bing"))

# Number of negative tweets
trump_tweet_sentiment <- tidy_trump_tweets %>%
  inner_join(get_sentiments("bing")) %>%
    count(created_at, sentiment) 

head(trump_tweet_sentiment)
```

Work with words and time.

```{r}
# Convert to datetime
tidy_trump_tweets$date <- as.Date(tidy_trump_tweets$created_at, 
                                          format="%Y-%m-%d %x")

# Only keep negative tweets and count them
trump_sentiment_plot <-
  tidy_trump_tweets %>%
    inner_join(get_sentiments("bing")) %>% 
      filter(sentiment=="negative") %>%
          count(date, sentiment)

# Negative tweets by day
ggplot(trump_sentiment_plot, aes(x=date, y=n))+
  geom_line(color="red", size=.5)+
    theme_minimal()+
    theme(axis.text.x = 
            element_text(angle = 60, hjust = 1, size=13))+
    theme(plot.title = 
            element_text(hjust = 0.5, size=18))+
      ylab("Number of Negative Words")+
      xlab("")+
      ggtitle("Negative Sentiment in Trump Tweets")+
      theme(aspect.ratio=1/4)
```

### Linguistic Inquiry Word Count

Psychometrics of text. Weights of every word that capture positive/negative, internal states, and so forth.

Typically a high-quality tool worth using, but the subtleties may not be captured. 

Note that false positives and false negative are common no matter what. 

Not sure which sentiment analysis to use? See Goncalves et al. (2013). Different sentiment dictionaries perform well in different contexts. The paper shows that no sentiment dictionary was as accurate as human coding.  